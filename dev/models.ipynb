{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Enron Spam Emails using Supervised Learning\n",
    "\n",
    "## DS-GA 1001: Introduction to Data Science Final Project\n",
    "\n",
    "### Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created On: 11/30/2020\n",
    "\n",
    "Modified On: 12/02/2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "This script establishes various supervised learning models for the `emails_cleaned.csv` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "We applied feature engineering to make the data ready for models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS! All modules have been imported.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import decomposition\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, mean_squared_error\n",
    "\n",
    "print(\"SUCCESS! All modules have been imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/emails_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X    0\n",
       "y    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove rows containing missing values\n",
    "df.dropna(subset=['X'], inplace=True)\n",
    "# Confirm that there is no missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model-ready dataset contains 785579 rows.\n"
     ]
    }
   ],
   "source": [
    "df.shape\n",
    "print('The model-ready dataset contains {} rows.'.format(df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "We applied [Term frequencyâ€“inverse document frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) (TF-IDF) method to transform string email contents into meaningful numeric figures so that they can be applied for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<785579x143084 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4292928 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a vectorization matrix using tf-idf vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorized_emails = vectorizer.fit_transform(df.X)\n",
    "vectorized_emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Test Split\n",
    "\n",
    "Before fitting the model, we splitted the dataset into two parts: train set and test set. We used train set to train models and used test set to examine the performance of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectorized_emails, df.y, test_size=test_size, random_state=88)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search Cross Validation\n",
    "\n",
    "We also applied grid search with cross validation to tune the hyperparameter of the logistic regression, `C`. The goal is to find the optimal hyperparameter `C` so that our model will reach the optimal complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logstic Regression with Elastic Net (Baseline)\n",
    "\n",
    "We used logistic regression as our baseline model. We also applied elastic net to weight coefficients and added a penalty term to our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Elastic Net\n",
    "\n",
    "We first applied grid search on the parameter of the elastic net, `l1 ratio`. We created a hyperparameter space that contains 30 possible l1 ratio values. We then fitted the training data with 5-fold cross validation to get the best l1 ratio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a hyperparameter grid for l1_ratio that is from 0 to 1\n",
    "l1_space = [0.2, 0.5, 0.8]\n",
    "param_grid = {'l1_ratio': l1_space}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_net = ElasticNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=ElasticNet(), n_jobs=-1,\n",
       "             param_grid={'l1_ratio': [0.2, 0.5, 0.8]})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup the grid search and fit the training data\n",
    "gm_cv = GridSearchCV(elastic_net, param_grid, cv=5, n_jobs=-1)\n",
    "gm_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned ElasticNet l1 ratio: {'l1_ratio': 0.2}\n",
      "Tuned ElasticNet R squared: -9.747972842921726e-06\n",
      "Tuned ElasticNet MSE: 0.2497127785371497\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test dataset and compute metrics\n",
    "y_pred = gm_cv.predict(X_test)\n",
    "r2 = gm_cv.score(X_test, y_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Tuned ElasticNet l1 ratio: {}\".format(gm_cv.best_params_))\n",
    "print(\"Tuned ElasticNet R squared: {}\".format(r2))\n",
    "print(\"Tuned ElasticNet MSE: {}\".format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_l1_ratio = gm_cv.best_params_['l1_ratio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization Strength\n",
    "\n",
    "We then applied another grid search method on the regularization parameter, `C`, of the logstic regression. A large `C` leads to overfit while a small `C` can get to underfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a hyperparameter grid for C that is from 0 to 1\n",
    "c_space = [0.01, 0.1, 1, 10, 100]\n",
    "param_grid = {'C': c_space}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a logistic regression model with elastic net\n",
    "logreg = LogisticRegression(solver='saga', penalty='elasticnet', l1_ratio=0.2, max_iter=5000, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_cv = GridSearchCV(logreg, param_grid, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "logreg_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param_logreg = logreg_cv.best_params_\n",
    "validation_auc_logreg = logreg_cv.best_score_\n",
    "\n",
    "y_pred = logreg_cv.predict_proba(x_test)[:, 1]\n",
    "test_auc_logreg = roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tuned Logistic Regression Parameters: {}'.format(best_param_logreg)\n",
    "print('Tuned Logstic Regression Validation AUC: {}'.format(validation_auc_logreg)\n",
    "print('Tuned Logstic Regression Test AUC: {}'.format(test_auc_logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a hyperparamter grid for random forest\n",
    "n_estimators = [50, 100, 200, 500]\n",
    "max_depth = [10, 50, 100, 200]\n",
    "min_samples_leaf = [10, 20 50, 100]\n",
    "param_grid = {'n_estimators': n_estimators, 'max_depth': max_depth, 'min_samples_leaf': min_samples_leaf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(criterion='entropy', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_cv = GridSearchCV(rf, param_grid, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "rf_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param_rf = rf_cv.best_params_\n",
    "validation_auc_rf = logreg_cv.best_score_\n",
    "\n",
    "y_pred = rf_cv.predict_proba(x_test)[:, 1]\n",
    "test_auc_rf = roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tuned Random Forest Parameters: {}'.format(best_param_rf)\n",
    "print('Tuned Random Forest Validation AUC: {}'.format(validation_auc_rf)\n",
    "print('Tuned Random Forest Test AUC: {}'.format(test_auc_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a hyperparameter grid for gradient boosting\n",
    "n_estimators = [100, 500]\n",
    "max_depth = [10, 100]\n",
    "param_grid = {'n_estimators': n_estimators, 'max_depth': max_depth}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gboost = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gboost_cv = GridSearchCV(gboost, param_grid, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "gboost_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param_gboost = gboost_cv.best_params_\n",
    "validation_auc_gboost = gboost_cv.best_score_\n",
    "\n",
    "y_pred = gboost_cv.predict_proba(x_test)[:, 1]\n",
    "test_auc_gboost = roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gboost_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tuned Gradient Boosting Parameters: {}'.format(best_param_gboost)\n",
    "print('Tuned Gradient Boosting Validation AUC: {}'.format(validation_auc_gboost)\n",
    "print('Tuned Gradient Boosting Test AUC: {}'.format(test_auc_gboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a hyperparameter grid for multilayer perceptron\n",
    "alpha = [0.01, 0.1, 1, 10]\n",
    "param_grid = {'alpha': alpha}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 30), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_cv = GridSearchCV(mlp, param_grid, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "mlp_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param_mlp = mlp_cv.best_params_\n",
    "validation_auc_mlp = mlp_cv.best_score_\n",
    "\n",
    "y_pred = mlp_cv.predict_proba(x_test)[:, 1]\n",
    "test_auc_mlp = roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tuned Multilayer Perceptron Parameters: {}'.format(best_param_mlp)\n",
    "print('Tuned Multilayer Perceptron Validation AUC: {}'.format(validation_auc_mlp)\n",
    "print('Tuned Multilayer Perceptron Test AUC: {}'.format(test_auc_mlp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
